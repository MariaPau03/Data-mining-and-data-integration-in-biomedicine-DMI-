{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8RzJQJMvwNJ7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaPau03/Data-mining-and-data-integration-in-biomedicine-DMI-/blob/main/AgenticAI2026__N1__LangChain_and_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LAB 1: Interacting with LLMs: LangChain and Groq**  \n",
        "*Introduction to AI Agents*, part of the *Data Mining and Data Integration in Biomedicine* course  \n",
        "Master in Bioinformatics for Health Sciences - 2026 edition\n",
        "Universitat Pompeu Fabra, Barcelona"
      ],
      "metadata": {
        "id": "UUZu4NlxolHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up environment"
      ],
      "metadata": {
        "id": "LylzCPYM7ZR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SETUP 1>** Install [langchain_groq](https://reference.langchain.com/python/integrations/langchain_groq/) python package"
      ],
      "metadata": {
        "id": "dO8ZH-o4pSze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KhMqUyfIoPOw",
        "outputId": "82dbb0ee-7c19-4645-cea4-d2c343af0b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-groq is a free to query a LLMs. Longchain is the frame of agentic AI"
      ],
      "metadata": {
        "id": "W99Z6p12_z3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SETUP 2>** Improt the python utility module form GitHub Gist [util_nb.py](https://gist.githubusercontent.com/frrepo33/0b3053ad66783e7397d4eb02c0385e08/raw/97bfdd36e2c600e51aa890595a5d1156bbaba8e4/util_nb.py)"
      ],
      "metadata": {
        "id": "ZYunBvva7ru3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from urllib.request import urlretrieve\n",
        "git_url = 'https://gist.githubusercontent.com/frrepo33/0b3053ad66783e7397d4eb02c0385e08/raw/547e69394aeb41fe3bf1efeee22e6c112dab336a/util_nb.py'\n",
        "urlretrieve(git_url, 'util_nb.py')\n",
        "from util_nb import *\n",
        "print(f'Module util_nb: {\"IMPORTED\" if 'util_nb' in sys.modules else \"NOT IMPORTED\"}')"
      ],
      "metadata": {
        "id": "W0FUlVbRrP7U",
        "outputId": "0142aeab-1a9a-43fc-a3aa-4ed22903541f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module util_nb: IMPORTED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SETUP 3>** Register to Groq and obtain Groq API key from [this link](https://console.groq.com/keys). Then copy to the [secret manager](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) of this Colab notebook.  \n",
        "*Extended info*: Groq [available models](https://console.groq.com/docs/models) and [rate limits](https://console.groq.com/docs/rate-limits), model sheets: [llama-3.1-8b-instant](https://console.groq.com/docs/model/llama-3.1-8b-instant), [llama-3.3-70b-versatile](https://console.groq.com/docs/model/llama-3.3-70b-versatile), [qwen/qwen3-32b](https://console.groq.com/docs/model/qwen/qwen3-32b)"
      ],
      "metadata": {
        "id": "7cDSkVsOpnBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "groq you can access models of generative language models. For each model you can see the speed, the price for input and output token. To ask to execute functions through an API, the agent will do the web search. Today we will see how to interact with this model using Longchain. To ask something by LLM to groq, we hsoukd generate an __API Key__. Then the popu-up link we will have to copy and paste it in the key section, paste in the value section and write a name for this. Then enable the check boss to enable the ntoebook to acces this API key. Is a secret manager that enables to acces the API key"
      ],
      "metadata": {
        "id": "keNCzfF8A90U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP 4> Read Groq API key from secret manager\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('groq-api-key')"
      ],
      "metadata": {
        "id": "vc9HwXq5qa6K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "store the API_key in a notebook environment variable"
      ],
      "metadata": {
        "id": "sSKuVM-SDlqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP 5> Import required modules\n",
        "import os\n",
        "\n",
        "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "mwqBHNrK8HDO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries that will enable the notebook to run the code"
      ],
      "metadata": {
        "id": "Uv3bUT4pDvWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting LLMs"
      ],
      "metadata": {
        "id": "nJoCPuJZ7eWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First initialize the LLM, provide the model_id, temperature, max_token value (nº of tokens being generated in order to not to pay a lot in a answer). __llama__ is a model from facebook."
      ],
      "metadata": {
        "id": "3jkq30dDD_fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1> Instantiate the model in LangChain\n",
        "model_id = \"llama-3.1-8b-instant\"\n",
        "llm = ChatGroq(\n",
        "    model=model_id,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ],
      "metadata": {
        "id": "eLZkbpSvqg5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How we can interact with the model? The interaction are framed as a set of messages classified in three types:"
      ],
      "metadata": {
        "id": "ET3SRM2XEkjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Extended info*: Overview of [Messages](https://docs.langchain.com/oss/python/langchain/messages) types in LangChain:  \n",
        "- **System message** - Tells the model how to behave and provide context for interactions; useful to set the tone, define the model’s role, and establish guidelines for responses.  Usually put in the chain on messages, to set a stage of a specific task.\n",
        "- **Human message** - Represents user input and interactions with the model and can contain text, images, audio, files, and any other amount of multimodal content. Can contain metadata (e.g. user_id to identify messages from the same user).  \n",
        "- **AI message** - Responses generated by the model, including multimodal data, and provider-specific metadata that you can later access (usage and response metadata).  \n",
        "\n",
        "Usually chat models accept a **sequence of message objects as input (of any kind)** and return an **AIMessage as output**. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages."
      ],
      "metadata": {
        "id": "0b-Ni2girRLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 2 ways to structure the initial prompt:"
      ],
      "metadata": {
        "id": "8Vwho4_jFAQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2> Explore distinct formats to specify the chain of prompt messages\n",
        "messages_data_struct = [\n",
        "    (\"system\", \"You are a helpful assistant that translates English to Spanish. Translate the user sentence.\"),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "] #Here we define a tuple with a message\n",
        "\n",
        "messages_class = [\n",
        "    SystemMessage(\"You are a helpful assistant that translates English to Spanish. Translate the user sentence.\"),\n",
        "    HumanMessage(\"I like running early morning.\")\n",
        "]"
      ],
      "metadata": {
        "id": "xF7ZFROZrLuD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llm has been instanciated before and will pass the message"
      ],
      "metadata": {
        "id": "ekMrbtFuFPYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3> Invoke the Generative Language Model\n",
        "ai_msg = llm.invoke(messages_class)"
      ],
      "metadata": {
        "id": "DFgCR6D4reNO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the print_var method is imported from the gisq"
      ],
      "metadata": {
        "id": "t6u5p-heFWpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4> Print the response\n",
        "print_var(ai_msg)"
      ],
      "metadata": {
        "id": "QF-etc6pr_3S",
        "outputId": "7e6283a5-6cc9-4116-f31a-e35f5838019e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   <class 'langchain_core.messages.ai.AIMessage'>\n",
            "     - content: 'La traducción es: Me gusta correr por la mañana temprano.'\n",
            "     - additional_kwargs: {}\n",
            "     - response_metadata: {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 57, 'total_tokens': 74, 'completion_time': 0.030256012, 'completion_tokens_details': None, 'prompt_time': 0.002771722, 'prompt_tokens_details': None, 'queue_time': 0.048069528, 'total_time': 0.033027734}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}\n",
            "     - type: 'ai'\n",
            "     - name: None\n",
            "     - id: 'lc_run--019c8a15-5cef-7ce1-9f31-ab29c4210225-0'\n",
            "     - tool_calls: []\n",
            "     - invalid_tool_calls: []\n",
            "     - usage_metadata: {'input_tokens': 57, 'output_tokens': 17, 'total_tokens': 74}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will be information and its content will give you the text translation and another metadata, for isntance the input token, output token and the final number of tokens, and in this cas, input tokens was 57 becasue if we summ the bumber of words in the input will sum up to 57. The total number of token is the sumof input and output token."
      ],
      "metadata": {
        "id": "T0dNmjGvFb8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Difference between reasoning and non-reasoning models"
      ],
      "metadata": {
        "id": "kCPbJw8muMME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM are trained for plan and understand how to execute a specific task, to generate text that describes this plan. Will compate the model trained for reasoning and another that cannot do."
      ],
      "metadata": {
        "id": "m7rQ7aDyFthk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1> Prompt that requires reasoning\n",
        "messages_class = [\n",
        "    HumanMessage(\"How many 'r's are in the word 'Strawberry'?\")\n",
        "]"
      ],
      "metadata": {
        "id": "jFqVBUMluZ9D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the non_reasoninig model is the same one as before"
      ],
      "metadata": {
        "id": "g9Q_a2AnGEGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2> Non-reasoning model\n",
        "model_id__non_reasoning = \"llama-3.1-8b-instant\"  # \"llama-3.3-70b-versatile\"\n",
        "llm = ChatGroq(\n",
        "    model=model_id__non_reasoning,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "ai_msg__non_reasoning = llm.invoke(messages_class)"
      ],
      "metadata": {
        "id": "edMKU3JMsgDs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ANSWER FROM NON-REASONING MODEL \"{model_id__non_reasoning}\":\\n{print_long_str(ai_msg__non_reasoning.content)}\\n\\n')\n",
        "print_var(ai_msg__non_reasoning)"
      ],
      "metadata": {
        "id": "iDs9VbEzvP8j",
        "outputId": "f31498f0-bc41-4e74-dd99-472ec453bd48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER FROM NON-REASONING MODEL \"llama-3.1-8b-instant\":\n",
            "To find the number of 'r's in the word 'Strawberry', let's break it down:  1. Straw - 0 'r's 2. berry - 1 'r'  So, the total number of 'r's\n",
            "in the word 'Strawberry' is 1.\n",
            "\n",
            "\n",
            "   <class 'langchain_core.messages.ai.AIMessage'>\n",
            "     - content: \"To find the number of 'r's in the word 'Strawberry', let's break it down:\\n\\n1. Straw - 0 'r's\\n2. berry - 1 'r'\\n\\nSo, the total number of 'r's in the word 'Strawberry' is 1.\"\n",
            "     - additional_kwargs: {}\n",
            "     - response_metadata: {'token_usage': {'completion_tokens': 63, 'prompt_tokens': 49, 'total_tokens': 112, 'completion_time': 0.083138785, 'completion_tokens_details': None, 'prompt_time': 0.00266784, 'prompt_tokens_details': None, 'queue_time': 0.045967819, 'total_time': 0.085806625}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}\n",
            "     - type: 'ai'\n",
            "     - name: None\n",
            "     - id: 'lc_run--019c8a18-70a3-7b22-9cd8-38320d5cc334-0'\n",
            "     - tool_calls: []\n",
            "     - invalid_tool_calls: []\n",
            "     - usage_metadata: {'input_tokens': 49, 'output_tokens': 63, 'total_tokens': 112}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3> Reasoning response\n",
        "model_id__reasoning = \"openai/gpt-oss-20b\"  # \"qwen/qwen3-32b\"\n",
        "llm = ChatGroq(\n",
        "    model=model_id__reasoning,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    reasoning_format=\"parsed\", #in the ai mesage split in two parts the reasoniing and the answer please\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")\n",
        "messages_class = [\n",
        "    HumanMessage(\"How many 'r's are in the word 'Strawberry'?\")\n",
        "]\n",
        "ai_msg__reasoning = llm.invoke(messages_class)"
      ],
      "metadata": {
        "id": "NWIjFggtunJC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ANSWER FROM REASONING MODEL \"{model_id__reasoning}\":\\n{print_long_str(ai_msg__reasoning.content)}\\n')\n",
        "print(f'REASONING CONTENT:\\n{print_long_str(ai_msg__reasoning.additional_kwargs[\"reasoning_content\"])}\\n')\n",
        "print_var(ai_msg__reasoning)"
      ],
      "metadata": {
        "id": "BOStcwNMvOwz",
        "outputId": "4a9b2143-3c99-456f-ab0e-16d4d13df5b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER FROM REASONING MODEL \"openai/gpt-oss-20b\":\n",
            "There are **3** 'r's in the word \"Strawberry\".\n",
            "\n",
            "REASONING CONTENT:\n",
            "We need to count the letter 'r' in the word 'Strawberry'. The word: S t r a w b e r r y. Let's count: 'S' no, 't' no, 'r' yes (1), 'a' no,\n",
            "'w' no, 'b' no, 'e' no, 'r' yes (2), 'r' yes (3), 'y' no. So 3 'r's. The answer: 3.\n",
            "\n",
            "   <class 'langchain_core.messages.ai.AIMessage'>\n",
            "     - content: 'There are **3** \\'r\\'s in the word \"Strawberry\".'\n",
            "     - additional_kwargs: {'reasoning_content': \"We need to count the letter 'r' in the word 'Strawberry'. The word: S t r a w b e r r y. Let's count: 'S' no, 't' no, 'r' yes (1), 'a' no, 'w' no, 'b' no, 'e' no, 'r' yes (2), 'r' yes (3), 'y' no. So 3 'r's. The answer: 3.\"}\n",
            "     - response_metadata: {'token_usage': {'completion_tokens': 129, 'prompt_tokens': 85, 'total_tokens': 214, 'completion_time': 0.131656815, 'completion_tokens_details': {'reasoning_tokens': 104}, 'prompt_time': 0.004124454, 'prompt_tokens_details': None, 'queue_time': 0.043822676, 'total_time': 0.135781269}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}\n",
            "     - type: 'ai'\n",
            "     - name: None\n",
            "     - id: 'lc_run--019c8a19-2dd8-73b0-8798-d64643b1be57-0'\n",
            "     - tool_calls: []\n",
            "     - invalid_tool_calls: []\n",
            "     - usage_metadata: {'input_tokens': 85, 'output_tokens': 129, 'total_tokens': 214, 'output_token_details': {'reasoning': 104}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HID6uhgoG2PH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodality: image to text  "
      ],
      "metadata": {
        "id": "8RzJQJMvwNJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image to describe --> [open image](https://hips.hearstapps.com/clv.h-cdn.co/assets/16/18/gettyimages-586890581.jpg)  \n",
        "*Extended info:* Model sheet from Grow: [llama-4-maverick-17b-128e-instruct](https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct); [Text and image prompt formatting](https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/#-llama-4-instruct-model-) and [image token generation](https://llama.developer.meta.com/docs/features/image-understanding#estimating-image-token-count) from Meta."
      ],
      "metadata": {
        "id": "YnssFNi092dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets images as input"
      ],
      "metadata": {
        "id": "ASJWnO_pH0CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id__multimodal = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        "llm = ChatGroq(\n",
        "    model=model_id__multimodal,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=1\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    (\"user\", [\n",
        "        {\"type\": \"text\", \"text\": \"Describe the content of this image in three sentences.\"},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://hips.hearstapps.com/clv.h-cdn.co/assets/16/18/gettyimages-586890581.jpg\"}}\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "ai_msg__multimodal = llm.invoke(messages)"
      ],
      "metadata": {
        "id": "i7T6x2lawQgq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Module util_nb: {\"IMPORTED\" if 'util_nb' in sys.modules else \"NOT IMPORTED\"}')\n",
        "print(f'ANSWER FROM MULTIMODAL MODEL \"{model_id__multimodal}\":\\n{print_long_str(ai_msg__multimodal.content)}\\n')\n",
        "print_var(ai_msg__multimodal)"
      ],
      "metadata": {
        "id": "12gDIjc_xory",
        "outputId": "9957c335-a912-4dee-d99a-4d586d01ec7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module util_nb: IMPORTED\n",
            "ANSWER FROM MULTIMODAL MODEL \"meta-llama/llama-4-maverick-17b-128e-instruct\":\n",
            "The image depicts a yellow Labrador Retriever standing on a beach, gazing to the right. The dog is positioned centrally in the frame, facing\n",
            "slightly to the right, with its head held high and ears relaxed. In the background, a body of water stretches out, with mountains visible on\n",
            "the opposite shore, their peaks capped with snow. The overall atmosphere of the image is one of serenity and tranquility, capturing a\n",
            "peaceful moment in time.\n",
            "\n",
            "   <class 'langchain_core.messages.ai.AIMessage'>\n",
            "     - content: 'The image depicts a yellow Labrador Retriever standing on a beach, gazing to the right. The dog is positioned centrally in the frame, facing slightly to the right, with its head held high and ears relaxed. In the background, a body of water stretches out, with mountains visible on the opposite shore, their peaks capped with snow. The overall atmosphere of the image is one of serenity and tranquility, capturing a peaceful moment in time.'\n",
            "     - additional_kwargs: {}\n",
            "     - response_metadata: {'token_usage': {'completion_tokens': 91, 'prompt_tokens': 2344, 'total_tokens': 2435, 'completion_time': 0.276869596, 'completion_tokens_details': None, 'prompt_time': 0.080905843, 'prompt_tokens_details': None, 'queue_time': 4.801362189, 'total_time': 0.357775439}, 'model_name': 'meta-llama/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': 'fp_217b6e6136', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}\n",
            "     - type: 'ai'\n",
            "     - name: None\n",
            "     - id: 'lc_run--019c8a1f-aeae-76f3-ac00-cf6fd47e8871-0'\n",
            "     - tool_calls: []\n",
            "     - invalid_tool_calls: []\n",
            "     - usage_metadata: {'input_tokens': 2344, 'output_tokens': 91, 'total_tokens': 2435}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unify the vectorial space between images and messages"
      ],
      "metadata": {
        "id": "cgmmVp8lIAu1"
      }
    }
  ]
}